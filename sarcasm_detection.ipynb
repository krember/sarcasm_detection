{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "sarcasm-detection.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ca4oUGXGaUO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function, division\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "import torch\n",
        "from torchtext import data\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FwFWStIGGaUf",
        "colab_type": "text"
      },
      "source": [
        "Read in the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dvQTPs89GaUl",
        "colab_type": "code",
        "colab": {},
        "outputId": "496f96ad-462a-4a37-b582-c369ba253d3e"
      },
      "source": [
        "df = pd.read_json('Sarcasm_Headlines_Dataset_v2.json', lines = True)\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>article_link</th>\n",
              "      <th>headline</th>\n",
              "      <th>is_sarcastic</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>https://www.theonion.com/thirtysomething-scien...</td>\n",
              "      <td>thirtysomething scientists unveil doomsday clo...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>https://www.huffingtonpost.com/entry/donna-edw...</td>\n",
              "      <td>dem rep. totally nails why congress is falling...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>https://www.huffingtonpost.com/entry/eat-your-...</td>\n",
              "      <td>eat your veggies: 9 deliciously different recipes</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>https://local.theonion.com/inclement-weather-p...</td>\n",
              "      <td>inclement weather prevents liar from getting t...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>https://www.theonion.com/mother-comes-pretty-c...</td>\n",
              "      <td>mother comes pretty close to using word 'strea...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                        article_link  \\\n",
              "0  https://www.theonion.com/thirtysomething-scien...   \n",
              "1  https://www.huffingtonpost.com/entry/donna-edw...   \n",
              "2  https://www.huffingtonpost.com/entry/eat-your-...   \n",
              "3  https://local.theonion.com/inclement-weather-p...   \n",
              "4  https://www.theonion.com/mother-comes-pretty-c...   \n",
              "\n",
              "                                            headline  is_sarcastic  \n",
              "0  thirtysomething scientists unveil doomsday clo...             1  \n",
              "1  dem rep. totally nails why congress is falling...             0  \n",
              "2  eat your veggies: 9 deliciously different recipes             0  \n",
              "3  inclement weather prevents liar from getting t...             1  \n",
              "4  mother comes pretty close to using word 'strea...             1  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-GYnUUssGaU1",
        "colab_type": "text"
      },
      "source": [
        "Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "riCPbSSxGaU4",
        "colab_type": "code",
        "colab": {},
        "outputId": "e1323cd7-e8cc-480b-bedb-e66779aae05b"
      },
      "source": [
        "df.drop(columns = 'article_link', inplace = True)\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>headline</th>\n",
              "      <th>is_sarcastic</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>thirtysomething scientists unveil doomsday clo...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>dem rep. totally nails why congress is falling...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>eat your veggies: 9 deliciously different recipes</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>inclement weather prevents liar from getting t...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>mother comes pretty close to using word 'strea...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            headline  is_sarcastic\n",
              "0  thirtysomething scientists unveil doomsday clo...             1\n",
              "1  dem rep. totally nails why congress is falling...             0\n",
              "2  eat your veggies: 9 deliciously different recipes             0\n",
              "3  inclement weather prevents liar from getting t...             1\n",
              "4  mother comes pretty close to using word 'strea...             1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "290DckQJGaVE",
        "colab_type": "code",
        "colab": {},
        "outputId": "1aa1f6cc-2920-48af-d8c1-cfe905ac525d"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(28619, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAM4InXUGaVU",
        "colab_type": "text"
      },
      "source": [
        "Save data frame to a text file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_style": "center",
        "id": "MRrOLzKjGaVV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.to_csv('sarcasm_headlines.txt', header = None, index = None, sep ='\\t', mode = 'a')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmn6J_xtGaVp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "outputId": "bd6122f9-b971-4e15-b4b8-e0eff728b023"
      },
      "source": [
        "!pip install -U sacremoses\n",
        "\n",
        "SEED = 421\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "TEXT = data.Field(tokenize='moses')\n",
        "LABEL = data.LabelField(dtype = torch.float)\n",
        "BATCH_SIZE = 32\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8d/ca/c0e81fc25a09ec0d4ea96c02801a8e8b5b77744acf4ad480481fca127fc6/sacremoses-0.0.31.tar.gz (802kB)\n",
            "\u001b[K     |████████████████████████████████| 808kB 43.0MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from sacremoses) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.6/dist-packages (from sacremoses) (7.0)\n",
            "Requirement already satisfied, skipping upgrade: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses) (0.13.2)\n",
            "Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.6/dist-packages (from sacremoses) (4.28.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.31-cp36-none-any.whl size=832904 sha256=bd4eea97d8454c1639533752f6006df940051afed074b03abecd7442b462d47a\n",
            "  Stored in directory: /root/.cache/pip/wheels/fa/0a/18/7c470ae4c30f82ff0f4e61ce4c0603ffcf609cbc033129e4de\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses\n",
            "Successfully installed sacremoses-0.0.31\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cLElWM2NGaVv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pos = data.TabularDataset(\n",
        "    path='sarcasm_headlines.txt', format='csv',\n",
        "    csv_reader_params={'delimiter':\"\\t\"},\n",
        "    fields=[('text', TEXT),\n",
        "            ('label', LABEL)])\n",
        "\n",
        "# Split data into 90/10 training/test\n",
        "trainandval, test_data=pos.split(split_ratio=0.90,random_state=random.seed(421))\n",
        "\n",
        "# Of the remaining training data, 80/20 train/validation\n",
        "train_data, valid_data = trainandval.split(split_ratio=0.80,random_state=random.seed(421))\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data),\n",
        "    batch_size=BATCH_SIZE,sort_key=lambda x: len(x.text),\n",
        "    device=device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LeFIRrywGaV1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers, bidirectional=bidirectional, \n",
        "                           dropout=dropout)\n",
        "        self.fc = nn.Linear(hidden_dim*2, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        embedded = self.dropout(self.embedding(x))\n",
        "        output, (hidden, cell) = self.rnn(embedded)\n",
        "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1))\n",
        "        return self.fc(hidden.squeeze(0))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQcUbWjWGaV6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Maximum vocabulary, choose word vectors\n",
        "TEXT.build_vocab(train_data,max_size=750, vectors=\"glove.twitter.27B.100d\")\n",
        "LABEL.build_vocab(train_data)\n",
        "#Network Hyperparameters\n",
        "INPUT_DIM = len(TEXT.vocab)\n",
        "EMBEDDING_DIM = 100\n",
        "HIDDEN_DIM = 128\n",
        "OUTPUT_DIM = 1\n",
        "N_LAYERS = 2\n",
        "BIDIRECTIONAL = True\n",
        "DROPOUT = 0.925"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZHAZBQpNG7Tg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, N_LAYERS, BIDIRECTIONAL, DROPOUT)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FouU-JgFG_on",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 148
        },
        "outputId": "102957fa-017c-4bab-ce5d-7346661e1c26"
      },
      "source": [
        "pretrained_embeddings = TEXT.vocab.vectors\n",
        "\n",
        "model.embedding.weight.data.copy_(pretrained_embeddings)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "        [ 0.8537,  0.2277,  0.6947,  ..., -0.0190,  0.1615,  0.2776],\n",
              "        ...,\n",
              "        [ 0.6397, -0.3348, -0.2175,  ..., -0.4405, -0.0152, -0.0467],\n",
              "        [ 0.6466,  0.1152,  0.4458,  ...,  0.3864,  0.1331,  0.3730],\n",
              "        [ 0.1481,  0.4934, -0.2522,  ...,  0.3715,  0.2636, -0.2121]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x7dp0astHTSC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = optim.Adam(model.parameters(),lr=0.001)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YpE3q8bCHsLB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def binary_accuracy(preds, y):\n",
        "  #round predictions to the closest integer\n",
        "  rounded_preds = torch.round(torch.sigmoid(preds))\n",
        "  correct = (rounded_preds == y).float() #convert into float for division\n",
        "  acc = correct.sum()/len(correct)\n",
        "  return acc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbbw6Of_Hzyx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "  epoch_loss = 0\n",
        "  epoch_acc = 0\n",
        "  model.train()\n",
        "  for batch in iterator:\n",
        "    optimizer.zero_grad()\n",
        "    predictions = model(batch.text).squeeze(1)\n",
        "    loss = criterion(predictions, batch.label)\n",
        "    acc = binary_accuracy(predictions, batch.label)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    epoch_loss += loss.item()\n",
        "    epoch_acc += acc.item()\n",
        "  return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZfdeoezH-xU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "  epoch_loss = 0\n",
        "  epoch_acc = 0\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    for batch in iterator:\n",
        "      predictions = model(batch.text).squeeze(1)\n",
        "      loss = criterion(predictions, batch.label)\n",
        "      acc = binary_accuracy(predictions, batch.label)\n",
        "      epoch_loss += loss.item()\n",
        "      epoch_acc += acc.item()\n",
        "  return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lt474EMHIFaw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f69bfd8d-d39d-49fe-e316-be466a419d36"
      },
      "source": [
        "N_EPOCHS=100\n",
        "bestmodelvalue=0\n",
        "for epoch in range(N_EPOCHS):\n",
        "  train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
        "  valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
        "  if valid_acc >= bestmodelvalue:\n",
        "    torch.save(model.state_dict(), \"sarcasm_detect_model.pt\")\n",
        "    bestmodelvalue=valid_acc\n",
        "  print(f'Epoch: {epoch+1:02} | Train Acc: {train_acc*100:.2f}% Val. Acc: {valid_acc*100:.2f}%')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Train Acc: 75.59% Val. Acc: 80.88%\n",
            "Epoch: 02 | Train Acc: 76.36% Val. Acc: 81.03%\n",
            "Epoch: 03 | Train Acc: 76.71% Val. Acc: 80.99%\n",
            "Epoch: 04 | Train Acc: 77.75% Val. Acc: 79.94%\n",
            "Epoch: 05 | Train Acc: 78.34% Val. Acc: 81.40%\n",
            "Epoch: 06 | Train Acc: 78.25% Val. Acc: 81.34%\n",
            "Epoch: 07 | Train Acc: 78.51% Val. Acc: 81.05%\n",
            "Epoch: 08 | Train Acc: 78.86% Val. Acc: 81.40%\n",
            "Epoch: 09 | Train Acc: 79.35% Val. Acc: 81.50%\n",
            "Epoch: 10 | Train Acc: 79.93% Val. Acc: 81.65%\n",
            "Epoch: 11 | Train Acc: 79.28% Val. Acc: 81.38%\n",
            "Epoch: 12 | Train Acc: 79.62% Val. Acc: 81.17%\n",
            "Epoch: 13 | Train Acc: 80.41% Val. Acc: 80.84%\n",
            "Epoch: 14 | Train Acc: 79.85% Val. Acc: 81.48%\n",
            "Epoch: 15 | Train Acc: 79.88% Val. Acc: 82.27%\n",
            "Epoch: 16 | Train Acc: 80.34% Val. Acc: 81.11%\n",
            "Epoch: 17 | Train Acc: 80.56% Val. Acc: 81.52%\n",
            "Epoch: 18 | Train Acc: 80.65% Val. Acc: 80.91%\n",
            "Epoch: 19 | Train Acc: 81.01% Val. Acc: 80.31%\n",
            "Epoch: 20 | Train Acc: 81.08% Val. Acc: 81.26%\n",
            "Epoch: 21 | Train Acc: 80.95% Val. Acc: 81.69%\n",
            "Epoch: 22 | Train Acc: 81.55% Val. Acc: 80.74%\n",
            "Epoch: 23 | Train Acc: 81.02% Val. Acc: 80.80%\n",
            "Epoch: 24 | Train Acc: 81.66% Val. Acc: 81.38%\n",
            "Epoch: 25 | Train Acc: 81.83% Val. Acc: 80.60%\n",
            "Epoch: 26 | Train Acc: 81.54% Val. Acc: 80.74%\n",
            "Epoch: 27 | Train Acc: 81.44% Val. Acc: 82.12%\n",
            "Epoch: 28 | Train Acc: 81.53% Val. Acc: 80.64%\n",
            "Epoch: 29 | Train Acc: 81.61% Val. Acc: 81.83%\n",
            "Epoch: 30 | Train Acc: 81.82% Val. Acc: 80.72%\n",
            "Epoch: 31 | Train Acc: 82.02% Val. Acc: 80.04%\n",
            "Epoch: 32 | Train Acc: 81.88% Val. Acc: 80.55%\n",
            "Epoch: 33 | Train Acc: 82.12% Val. Acc: 80.86%\n",
            "Epoch: 34 | Train Acc: 82.45% Val. Acc: 79.89%\n",
            "Epoch: 35 | Train Acc: 82.40% Val. Acc: 80.49%\n",
            "Epoch: 36 | Train Acc: 82.13% Val. Acc: 81.58%\n",
            "Epoch: 37 | Train Acc: 82.74% Val. Acc: 81.05%\n",
            "Epoch: 38 | Train Acc: 82.63% Val. Acc: 81.38%\n",
            "Epoch: 39 | Train Acc: 82.48% Val. Acc: 80.51%\n",
            "Epoch: 40 | Train Acc: 82.25% Val. Acc: 80.18%\n",
            "Epoch: 41 | Train Acc: 82.97% Val. Acc: 80.60%\n",
            "Epoch: 42 | Train Acc: 82.23% Val. Acc: 80.72%\n",
            "Epoch: 43 | Train Acc: 82.45% Val. Acc: 81.05%\n",
            "Epoch: 44 | Train Acc: 82.31% Val. Acc: 79.98%\n",
            "Epoch: 45 | Train Acc: 82.91% Val. Acc: 81.23%\n",
            "Epoch: 46 | Train Acc: 82.63% Val. Acc: 81.11%\n",
            "Epoch: 47 | Train Acc: 82.72% Val. Acc: 80.64%\n",
            "Epoch: 48 | Train Acc: 83.12% Val. Acc: 80.78%\n",
            "Epoch: 49 | Train Acc: 82.61% Val. Acc: 81.38%\n",
            "Epoch: 50 | Train Acc: 82.89% Val. Acc: 80.12%\n",
            "Epoch: 51 | Train Acc: 82.78% Val. Acc: 80.39%\n",
            "Epoch: 52 | Train Acc: 82.66% Val. Acc: 80.74%\n",
            "Epoch: 53 | Train Acc: 82.91% Val. Acc: 81.44%\n",
            "Epoch: 54 | Train Acc: 83.27% Val. Acc: 80.64%\n",
            "Epoch: 55 | Train Acc: 83.15% Val. Acc: 80.68%\n",
            "Epoch: 56 | Train Acc: 83.02% Val. Acc: 80.84%\n",
            "Epoch: 57 | Train Acc: 82.86% Val. Acc: 80.66%\n",
            "Epoch: 58 | Train Acc: 83.21% Val. Acc: 80.02%\n",
            "Epoch: 59 | Train Acc: 83.17% Val. Acc: 79.65%\n",
            "Epoch: 60 | Train Acc: 82.58% Val. Acc: 80.18%\n",
            "Epoch: 61 | Train Acc: 83.21% Val. Acc: 80.47%\n",
            "Epoch: 62 | Train Acc: 83.09% Val. Acc: 78.04%\n",
            "Epoch: 63 | Train Acc: 83.07% Val. Acc: 80.43%\n",
            "Epoch: 64 | Train Acc: 83.00% Val. Acc: 81.24%\n",
            "Epoch: 65 | Train Acc: 83.18% Val. Acc: 81.19%\n",
            "Epoch: 66 | Train Acc: 83.41% Val. Acc: 80.18%\n",
            "Epoch: 67 | Train Acc: 82.96% Val. Acc: 81.15%\n",
            "Epoch: 68 | Train Acc: 83.04% Val. Acc: 81.48%\n",
            "Epoch: 69 | Train Acc: 83.30% Val. Acc: 81.36%\n",
            "Epoch: 70 | Train Acc: 83.17% Val. Acc: 80.26%\n",
            "Epoch: 71 | Train Acc: 83.00% Val. Acc: 80.49%\n",
            "Epoch: 72 | Train Acc: 83.19% Val. Acc: 80.08%\n",
            "Epoch: 73 | Train Acc: 83.51% Val. Acc: 78.66%\n",
            "Epoch: 74 | Train Acc: 83.23% Val. Acc: 79.85%\n",
            "Epoch: 75 | Train Acc: 83.00% Val. Acc: 81.67%\n",
            "Epoch: 76 | Train Acc: 83.26% Val. Acc: 80.45%\n",
            "Epoch: 77 | Train Acc: 83.45% Val. Acc: 82.14%\n",
            "Epoch: 78 | Train Acc: 83.18% Val. Acc: 81.11%\n",
            "Epoch: 79 | Train Acc: 83.55% Val. Acc: 80.76%\n",
            "Epoch: 80 | Train Acc: 83.54% Val. Acc: 81.34%\n",
            "Epoch: 81 | Train Acc: 83.51% Val. Acc: 80.64%\n",
            "Epoch: 82 | Train Acc: 83.65% Val. Acc: 82.25%\n",
            "Epoch: 83 | Train Acc: 83.22% Val. Acc: 81.26%\n",
            "Epoch: 84 | Train Acc: 83.45% Val. Acc: 81.81%\n",
            "Epoch: 85 | Train Acc: 83.50% Val. Acc: 81.48%\n",
            "Epoch: 86 | Train Acc: 83.44% Val. Acc: 79.23%\n",
            "Epoch: 87 | Train Acc: 83.62% Val. Acc: 80.45%\n",
            "Epoch: 88 | Train Acc: 83.33% Val. Acc: 79.96%\n",
            "Epoch: 89 | Train Acc: 83.24% Val. Acc: 79.77%\n",
            "Epoch: 90 | Train Acc: 83.24% Val. Acc: 81.42%\n",
            "Epoch: 91 | Train Acc: 83.24% Val. Acc: 81.23%\n",
            "Epoch: 92 | Train Acc: 83.61% Val. Acc: 81.23%\n",
            "Epoch: 93 | Train Acc: 83.30% Val. Acc: 81.90%\n",
            "Epoch: 94 | Train Acc: 83.43% Val. Acc: 80.88%\n",
            "Epoch: 95 | Train Acc: 83.32% Val. Acc: 79.91%\n",
            "Epoch: 96 | Train Acc: 83.31% Val. Acc: 81.77%\n",
            "Epoch: 97 | Train Acc: 83.52% Val. Acc: 82.29%\n",
            "Epoch: 98 | Train Acc: 83.56% Val. Acc: 79.91%\n",
            "Epoch: 99 | Train Acc: 83.57% Val. Acc: 78.41%\n",
            "Epoch: 100 | Train Acc: 83.30% Val. Acc: 78.97%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROrm0_swIQHW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "8d8ad0c8-663a-4411-d70e-5911b58463b7"
      },
      "source": [
        "model.load_state_dict(torch.load(\"sarcasm_detect_model.pt\"))\n",
        "model.eval()\n",
        "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
        "print(\"Test Accuracy: \",test_acc)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy:  0.8233134925365448\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fzJlUGQWKuXQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}